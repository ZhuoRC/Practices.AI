#!/usr/bin/env python3
"""
DeepSeek-OCR RESTful API Server (Low VRAM Version)
DeepSeek-OCR RESTful API æœåŠ¡å™¨ï¼ˆä½æ˜¾å­˜ä¼˜åŒ–ç‰ˆæœ¬ï¼‰

Optimized for GPUs with 6GB+ VRAM
é’ˆå¯¹ 6GB+ æ˜¾å­˜çš„ GPU ä¼˜åŒ–

Features / åŠŸèƒ½:
- OCR text extraction / OCR æ–‡æœ¬æå–
- Save results to file / ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
- Support for file upload and Base64 input / æ”¯æŒæ–‡ä»¶ä¸Šä¼ å’Œ Base64 è¾“å…¥
"""
from fastapi import FastAPI, HTTPException, File, UploadFile, Form
from fastapi.responses import Response, FileResponse
from pydantic import BaseModel, Field
from typing import Optional
from contextlib import asynccontextmanager
import torch
from transformers import AutoModel, AutoTokenizer
import io
import base64
from PIL import Image
import uvicorn
import logging
import gc
import os
from datetime import datetime

# Setup logging / è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Suppress harmless transformers warnings / æŠ‘åˆ¶æ— å®³çš„ transformers è­¦å‘Š
import warnings
warnings.filterwarnings('ignore', message='.*do_sample.*temperature.*')
warnings.filterwarnings('ignore', message='.*attention mask.*pad token.*')
warnings.filterwarnings('ignore', message='.*seen_tokens.*deprecated.*')
warnings.filterwarnings('ignore', message='.*get_max_cache.*deprecated.*')
warnings.filterwarnings('ignore', message='.*position_ids.*position_embeddings.*')

# Import the masked_scatter fix
try:
    from fix_masked_scatter import patch_deepseek_ocr_model
    PATCH_AVAILABLE = True
    logger.info("âœ“ masked_scatter fix module loaded / masked_scatter ä¿®å¤æ¨¡å—å·²åŠ è½½")
except ImportError:
    PATCH_AVAILABLE = False
    logger.warning("âš  fix_masked_scatter.py not found, running without patch")
    logger.warning("âš  æœªæ‰¾åˆ° fix_masked_scatter.pyï¼Œåœ¨æ²¡æœ‰è¡¥ä¸çš„æƒ…å†µä¸‹è¿è¡Œ")

# Create directories / åˆ›å»ºç›®å½•
INPUT_DIR = os.path.join(os.path.dirname(__file__), "input")
OUTPUT_DIR = os.path.join(os.path.dirname(__file__), "output")
os.makedirs(INPUT_DIR, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)
logger.info(f"Input images directory / è¾“å…¥å›¾ç‰‡ç›®å½•: {INPUT_DIR}")
logger.info(f"Output results directory / è¾“å‡ºç»“æœç›®å½•: {OUTPUT_DIR}")

# Global variables to store model and tokenizer / å­˜å‚¨æ¨¡å‹å’Œåˆ†è¯å™¨çš„å…¨å±€å˜é‡
model = None
tokenizer = None

class OCRRequest(BaseModel):
    """Request model for OCR / OCR è¯·æ±‚æ¨¡å‹"""
    image_base64: str = Field(..., description="Base64 encoded image / Base64 ç¼–ç çš„å›¾ç‰‡")
    prompt: Optional[str] = Field(default="<image>\nFree OCR.", description="OCR prompt / OCR æç¤ºè¯")
    save_to_file: Optional[bool] = Field(default=True, description="Save result to file / ä¿å­˜ç»“æœåˆ°æ–‡ä»¶")
    base_size: Optional[int] = Field(default=1024, description="Base size for encoding (512-1280, balanced quality) / ç¼–ç åŸºç¡€å°ºå¯¸ï¼ˆ512-1280ï¼Œå¹³è¡¡è´¨é‡ï¼‰")
    image_size: Optional[int] = Field(default=896, description="Image processing size (512-1280, balanced quality) / å›¾åƒå¤„ç†å°ºå¯¸ï¼ˆ512-1280ï¼Œå¹³è¡¡è´¨é‡ï¼‰")
    crop_mode: Optional[bool] = Field(default=True, description="Enable document boundary detection / å¯ç”¨æ–‡æ¡£è¾¹ç•Œæ£€æµ‹")

class OCRResponse(BaseModel):
    """Response model for OCR / OCR å“åº”æ¨¡å‹"""
    success: bool
    message: str
    text: Optional[str] = None
    output_file: Optional[str] = None

def cleanup_memory():
    """Aggressive memory cleanup / æ¿€è¿›çš„å†…å­˜æ¸…ç†"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        torch.cuda.ipc_collect()

def decode_base64_image(image_base64: str) -> Image.Image:
    """
    Decode base64 string to PIL Image
    å°† base64 å­—ç¬¦ä¸²è§£ç ä¸º PIL å›¾ç‰‡
    """
    try:
        # Remove data URL prefix if present / ç§»é™¤ data URL å‰ç¼€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if ',' in image_base64:
            image_base64 = image_base64.split(',')[1]

        image_bytes = base64.b64decode(image_base64)
        image = Image.open(io.BytesIO(image_bytes))

        # Convert to RGB if necessary / å¦‚æœéœ€è¦ï¼Œè½¬æ¢ä¸º RGB
        if image.mode != 'RGB':
            image = image.convert('RGB')

        return image
    except Exception as e:
        raise ValueError(f"Failed to decode base64 image / è§£ç  base64 å›¾ç‰‡å¤±è´¥: {str(e)}")

def save_image(image: Image.Image, prefix: str = "input") -> str:
    """
    Save image to input directory
    ä¿å­˜å›¾ç‰‡åˆ°è¾“å…¥ç›®å½•
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{prefix}_{timestamp}.png"
    filepath = os.path.join(INPUT_DIR, filename)
    image.save(filepath)
    logger.info(f"Saved image / å·²ä¿å­˜å›¾ç‰‡: {filepath}")
    return filepath

def save_text_result(text: str, prefix: str = "ocr_result") -> str:
    """
    Save OCR result to output directory
    ä¿å­˜ OCR ç»“æœåˆ°è¾“å‡ºç›®å½•
    """
    # Handle None or empty text / å¤„ç† None æˆ–ç©ºæ–‡æœ¬
    if text is None:
        text = "[No text extracted / æœªæå–åˆ°æ–‡æœ¬]"
    elif not text.strip():
        text = "[Empty result / ç©ºç»“æœ]"

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{prefix}_{timestamp}.txt"
    filepath = os.path.join(OUTPUT_DIR, filename)

    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(text)

    logger.info(f"Saved OCR result / å·²ä¿å­˜ OCR ç»“æœ: {filepath}")
    return filename

@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Load the DeepSeek-OCR model with memory optimizations
    åŠ è½½ DeepSeek-OCR æ¨¡å‹å¹¶è¿›è¡Œå†…å­˜ä¼˜åŒ–
    """
    global model, tokenizer

    try:
        logger.info("=" * 70)
        logger.info("Loading DeepSeek-OCR model (LOW VRAM MODE)")
        logger.info("åŠ è½½ DeepSeek-OCR æ¨¡å‹ï¼ˆä½æ˜¾å­˜æ¨¡å¼ï¼‰")
        logger.info("=" * 70)

        model_name = 'deepseek-ai/DeepSeek-OCR'

        # Check GPU / æ£€æŸ¥ GPU
        if torch.cuda.is_available():
            device = "cuda"
            gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3
            logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
            logger.info(f"Total GPU Memory / æ€»æ˜¾å­˜: {gpu_mem:.1f} GB")

            # Use bfloat16 for memory efficiency / ä½¿ç”¨ bfloat16 ä»¥èŠ‚çœå†…å­˜
            torch_dtype = torch.bfloat16
            logger.info("Using torch.bfloat16 for memory efficiency")
            logger.info("ä½¿ç”¨ torch.bfloat16 èŠ‚çœå†…å­˜")
        else:
            torch_dtype = torch.float32
            device = "cpu"
            logger.info("GPU not available, using CPU")
            logger.info("GPU ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU")

        # Load tokenizer / åŠ è½½åˆ†è¯å™¨
        logger.info("\nLoading tokenizer... / åŠ è½½åˆ†è¯å™¨...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True
        )
        logger.info("âœ“ Tokenizer loaded successfully / åˆ†è¯å™¨åŠ è½½æˆåŠŸ")

        # Load model / åŠ è½½æ¨¡å‹
        logger.info("\nLoading model... / åŠ è½½æ¨¡å‹...")
        logger.info("(This may take several minutes on first run)")
        logger.info("ï¼ˆé¦–æ¬¡è¿è¡Œå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰")

        # Try to use flash_attention_2, fallback to eager if not available
        # å°è¯•ä½¿ç”¨ flash_attention_2ï¼Œå¦‚æœä¸å¯ç”¨åˆ™å›é€€åˆ° eager
        attn_impl = 'flash_attention_2'
        try:
            logger.info(f"Attempting to load with {attn_impl}...")
            model = AutoModel.from_pretrained(
                model_name,
                attn_implementation=attn_impl,
                trust_remote_code=True,
                use_safetensors=True,
                torch_dtype=torch_dtype
            )
            logger.info(f"âœ“ Model loaded with {attn_impl}")
        except Exception as e:
            logger.warning(f"Failed to load with flash_attention_2: {e}")
            logger.info("Falling back to eager attention implementation...")
            logger.info("å›é€€åˆ° eager æ³¨æ„åŠ›å®ç°...")
            attn_impl = 'eager'
            model = AutoModel.from_pretrained(
                model_name,
                attn_implementation=attn_impl,
                trust_remote_code=True,
                use_safetensors=True,
                torch_dtype=torch_dtype
            )
            logger.info("âœ“ Model loaded with eager attention (slower but works without flash-attn)")
            logger.info("âœ“ æ¨¡å‹å·²åŠ è½½ï¼Œä½¿ç”¨ eager æ³¨æ„åŠ›ï¼ˆè¾ƒæ…¢ä½†æ— éœ€ flash-attnï¼‰")

        # Set model to evaluation mode / è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
        model = model.eval()

        if torch.cuda.is_available():
            logger.info("\n" + "=" * 70)
            logger.info("Applying memory optimizations:")
            logger.info("åº”ç”¨å†…å­˜ä¼˜åŒ–:")
            logger.info("=" * 70)

            # Move model to GPU / å°†æ¨¡å‹ç§»è‡³ GPU
            model = model.cuda()
            logger.info("âœ“ Model moved to GPU / æ¨¡å‹å·²ç§»è‡³ GPU")

            # Use CPU offload for large models if needed / å¦‚éœ€è¦ï¼Œå¯¹å¤§æ¨¡å‹ä½¿ç”¨ CPU å¸è½½
            # model = model.to(device)

            # Clean up / æ¸…ç†
            cleanup_memory()
            logger.info("âœ“ Initial memory cleanup completed / åˆå§‹å†…å­˜æ¸…ç†å®Œæˆ")

        else:
            model = model.to(device)

        logger.info("\n" + "=" * 70)
        logger.info("Model loaded successfully! / æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        logger.info("=" * 70)

        # Apply masked_scatter fix patch
        if PATCH_AVAILABLE:
            try:
                logger.info("\nApplying masked_scatter_ fix / åº”ç”¨ masked_scatter_ ä¿®å¤")
                model = patch_deepseek_ocr_model(model)
                logger.info("âœ“ Patch applied successfully / è¡¥ä¸åº”ç”¨æˆåŠŸ")
            except Exception as e:
                logger.warning(f"âš  Failed to apply patch / åº”ç”¨è¡¥ä¸å¤±è´¥: {e}")
                logger.warning("  Model will run without patch (may encounter CUDA errors)")
                logger.warning("  æ¨¡å‹å°†åœ¨æ²¡æœ‰è¡¥ä¸çš„æƒ…å†µä¸‹è¿è¡Œï¼ˆå¯èƒ½é‡åˆ° CUDA é”™è¯¯ï¼‰")
        else:
            logger.warning("\nâš  Running without masked_scatter_ fix")
            logger.warning("âš  åœ¨æ²¡æœ‰ masked_scatter_ ä¿®å¤çš„æƒ…å†µä¸‹è¿è¡Œ")
            logger.warning("  If you encounter CUDA assertion errors, ensure fix_masked_scatter.py is present")
            logger.warning("  å¦‚æœé‡åˆ° CUDA æ–­è¨€é”™è¯¯ï¼Œè¯·ç¡®ä¿ fix_masked_scatter.py å­˜åœ¨")

        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated(0) / 1024**3
            reserved = torch.cuda.memory_reserved(0) / 1024**3
            logger.info(f"\nGPU memory allocated / GPU å·²åˆ†é…å†…å­˜: {allocated:.2f} GB")
            logger.info(f"GPU memory reserved / GPU å·²ä¿ç•™å†…å­˜: {reserved:.2f} GB")

    except Exception as e:
        logger.error(f"\nâŒ Failed to load model / æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        logger.error("\nTroubleshooting tips / æ•…éšœæ’é™¤æç¤º:")
        logger.error("1. Ensure you have at least 6GB free GPU memory")
        logger.error("   ç¡®ä¿è‡³å°‘æœ‰ 6GB å¯ç”¨ GPU å†…å­˜")
        logger.error("2. Install flash-attn: pip install flash-attn==2.7.3 --no-build-isolation")
        logger.error("   å®‰è£… flash-attn: pip install flash-attn==2.7.3 --no-build-isolation")
        logger.error("3. Close all other GPU applications")
        logger.error("   å…³é—­æ‰€æœ‰å…¶ä»– GPU åº”ç”¨ç¨‹åº")
        logger.error("4. Check requirements with: python check_gpu.py")
        logger.error("   ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ£€æŸ¥è¦æ±‚: python check_gpu.py")
        raise

    # Yield control back to the application / å°†æ§åˆ¶æƒäº¤è¿˜ç»™åº”ç”¨ç¨‹åº
    yield

    # Cleanup on shutdown / å…³é—­æ—¶æ¸…ç†
    logger.info("Shutting down... / æ­£åœ¨å…³é—­...")
    cleanup_memory()

# Initialize FastAPI app with lifespan / ä½¿ç”¨ lifespan åˆå§‹åŒ– FastAPI åº”ç”¨
app = FastAPI(
    title="DeepSeek-OCR API (Low VRAM)",
    description="RESTful API for DeepSeek-OCR optimized for low VRAM GPUs (6GB+) / é’ˆå¯¹ä½æ˜¾å­˜ GPUï¼ˆ6GB+ï¼‰ä¼˜åŒ–çš„ DeepSeek-OCR RESTful API",
    version="1.0.0",
    lifespan=lifespan
)

@app.get("/")
async def root():
    """Root endpoint with API information / åŒ…å« API ä¿¡æ¯çš„æ ¹ç«¯ç‚¹"""
    return {
        "name": "DeepSeek-OCR API (Low VRAM Mode)",
        "name_zh": "DeepSeek-OCR APIï¼ˆä½æ˜¾å­˜æ¨¡å¼ï¼‰",
        "status": "running",
        "model": "deepseek-ai/DeepSeek-OCR",
        "mode": "Low VRAM Optimized",
        "mode_zh": "ä½æ˜¾å­˜ä¼˜åŒ–",
        "optimizations": [
            "torch.bfloat16 precision",
            "Flash Attention 2",
            "Aggressive memory cleanup",
            "CPU offload (if needed)"
        ],
        "optimizations_zh": [
            "torch.bfloat16 ç²¾åº¦",
            "Flash Attention 2",
            "æ¿€è¿›çš„å†…å­˜æ¸…ç†",
            "CPU å¸è½½ï¼ˆå¦‚éœ€è¦ï¼‰"
        ],
        "endpoints": {
            "/ocr": "POST - Extract text from image (file upload) / ä»å›¾ç‰‡ä¸­æå–æ–‡æœ¬ï¼ˆæ–‡ä»¶ä¸Šä¼ ï¼‰",
            "/ocr-base64": "POST - Extract text from base64 image / ä» base64 å›¾ç‰‡ä¸­æå–æ–‡æœ¬",
            "/health": "GET - Health check with memory stats / å¥åº·æ£€æŸ¥å’Œå†…å­˜ç»Ÿè®¡",
            "/results/{filename}": "GET - Download saved OCR result / ä¸‹è½½ä¿å­˜çš„ OCR ç»“æœ",
            "/cleanup": "POST - Force cleanup GPU memory / å¼ºåˆ¶æ¸…ç† GPU å†…å­˜",
            "/docs": "GET - API documentation / API æ–‡æ¡£"
        }
    }

@app.get("/health")
async def health_check():
    """Health check endpoint with memory statistics / å¸¦å†…å­˜ç»Ÿè®¡çš„å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    if model is None or tokenizer is None:
        raise HTTPException(
            status_code=503,
            detail="Model not loaded / æ¨¡å‹æœªåŠ è½½"
        )

    device = "cuda" if torch.cuda.is_available() else "cpu"
    health_info = {
        "status": "healthy",
        "model_loaded": model is not None,
        "tokenizer_loaded": tokenizer is not None,
        "device": device,
        "mode": "Low VRAM (torch.bfloat16)" if device == "cuda" else "CPU"
    }

    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated(0) / 1024**3
        reserved = torch.cuda.memory_reserved(0) / 1024**3
        total = torch.cuda.get_device_properties(0).total_memory / 1024**3

        health_info.update({
            "gpu_name": torch.cuda.get_device_name(0),
            "gpu_memory_total": f"{total:.2f} GB",
            "gpu_memory_allocated": f"{allocated:.2f} GB",
            "gpu_memory_reserved": f"{reserved:.2f} GB",
            "gpu_memory_free": f"{total - reserved:.2f} GB"
        })

    return health_info

@app.post("/cleanup")
async def cleanup():
    """Force aggressive memory cleanup / å¼ºåˆ¶æ¿€è¿›çš„å†…å­˜æ¸…ç†"""
    logger.info("Performing aggressive memory cleanup... / æ‰§è¡Œæ¿€è¿›çš„å†…å­˜æ¸…ç†...")
    cleanup_memory()

    result = {
        "message": "Memory cleanup completed / å†…å­˜æ¸…ç†å®Œæˆ"
    }

    if torch.cuda.is_available():
        result.update({
            "gpu_memory_allocated": f"{torch.cuda.memory_allocated(0) / 1024**3:.2f} GB",
            "gpu_memory_reserved": f"{torch.cuda.memory_reserved(0) / 1024**3:.2f} GB"
        })

    return result

@app.post("/ocr", response_model=OCRResponse)
async def ocr_from_file(
    image: UploadFile = File(..., description="Image file for OCR / ç”¨äº OCR çš„å›¾ç‰‡æ–‡ä»¶"),
    prompt: Optional[str] = Form(default="<image>\nFree OCR.", description="OCR prompt / OCR æç¤ºè¯"),
    save_to_file: Optional[bool] = Form(default=True, description="Save result to file / ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"),
    base_size: Optional[int] = Form(default=1024, description="Base size for encoding (512-1280, balanced quality) / ç¼–ç åŸºç¡€å°ºå¯¸ï¼ˆ512-1280ï¼Œå¹³è¡¡è´¨é‡ï¼‰"),
    image_size: Optional[int] = Form(default=896, description="Image processing size (512-1280, balanced quality) / å›¾åƒå¤„ç†å°ºå¯¸ï¼ˆ512-1280ï¼Œå¹³è¡¡è´¨é‡ï¼‰"),
    crop_mode: Optional[bool] = Form(default=True, description="Enable document boundary detection / å¯ç”¨æ–‡æ¡£è¾¹ç•Œæ£€æµ‹")
):
    """
    Extract text from uploaded image file (LOW VRAM version)
    ä»ä¸Šä¼ çš„å›¾ç‰‡æ–‡ä»¶ä¸­æå–æ–‡æœ¬ï¼ˆä½æ˜¾å­˜ç‰ˆæœ¬ï¼‰

    Optimized for GPUs with 6GB+ VRAM
    é’ˆå¯¹ 6GB+ æ˜¾å­˜çš„ GPU ä¼˜åŒ–
    """
    if model is None or tokenizer is None:
        raise HTTPException(
            status_code=503,
            detail="Model not loaded / æ¨¡å‹æœªåŠ è½½"
        )

    try:
        # Read and decode image / è¯»å–å¹¶è§£ç å›¾ç‰‡
        logger.info("\n" + "=" * 70)
        logger.info("Starting OCR (LOW VRAM MODE) / å¼€å§‹ OCRï¼ˆä½æ˜¾å­˜æ¨¡å¼ï¼‰")
        logger.info("=" * 70)

        # Cleanup before processing / å¤„ç†å‰æ¸…ç†
        cleanup_memory()

        # Read uploaded file / è¯»å–ä¸Šä¼ çš„æ–‡ä»¶
        image_bytes = await image.read()
        pil_image = Image.open(io.BytesIO(image_bytes))

        # Convert to RGB if necessary / å¦‚éœ€è¦ï¼Œè½¬æ¢ä¸º RGB
        if pil_image.mode != 'RGB':
            pil_image = pil_image.convert('RGB')

        logger.info(f"Image size / å›¾ç‰‡å°ºå¯¸: {pil_image.size}")
        logger.info(f"Prompt / æç¤ºè¯: {prompt}")

        # Save input image and get filepath / ä¿å­˜è¾“å…¥å›¾ç‰‡å¹¶è·å–æ–‡ä»¶è·¯å¾„
        image_filepath = save_image(pil_image, prefix="input")

        if torch.cuda.is_available():
            mem_before = torch.cuda.memory_allocated(0) / 1024**3
            logger.info(f"GPU memory before / GPU å†…å­˜ä½¿ç”¨ï¼ˆå‰ï¼‰: {mem_before:.2f} GB")

        # Perform OCR / æ‰§è¡Œ OCR
        logger.info("\nPerforming OCR... / æ‰§è¡Œ OCR...")
        logger.info(f"Settings / è®¾ç½®: base_size={base_size}, image_size={image_size}, crop_mode={crop_mode}")

        # Capture stdout to get the model's printed output
        # æ•è·æ ‡å‡†è¾“å‡ºä»¥è·å–æ¨¡å‹æ‰“å°çš„è¾“å‡º
        import io as io_module
        import sys

        old_stdout = sys.stdout
        sys.stdout = captured_output = io_module.StringIO()

        try:
            with torch.inference_mode():
                # Run inference using model.infer() / ä½¿ç”¨ model.infer() è¿è¡Œæ¨ç†
                # Use save_results=False to get return value / ä½¿ç”¨ save_results=False ä»¥è·å–è¿”å›å€¼
                logger.info("Starting model inference... / å¼€å§‹æ¨¡å‹æ¨ç†...")
                logger.info("This may take 1-3 minutes for large images / å¤§å›¾ç‰‡å¯èƒ½éœ€è¦ 1-3 åˆ†é’Ÿ")

                result_text = model.infer(
                    tokenizer,
                    prompt=prompt,
                    image_file=image_filepath,
                    output_path=OUTPUT_DIR,  # Use output directory / ä½¿ç”¨è¾“å‡ºç›®å½•
                    base_size=base_size,     # Higher values = better quality / æ›´é«˜çš„å€¼ = æ›´å¥½çš„è´¨é‡
                    image_size=image_size,   # Higher values = better quality / æ›´é«˜çš„å€¼ = æ›´å¥½çš„è´¨é‡
                    crop_mode=crop_mode,     # Enable boundary detection / å¯ç”¨è¾¹ç•Œæ£€æµ‹
                    save_results=False       # Return result instead of just saving / è¿”å›ç»“æœè€Œä¸ä»…ä»…ä¿å­˜
                )

                logger.info("Model inference completed / æ¨¡å‹æ¨ç†å®Œæˆ")
        finally:
            # Restore stdout
            sys.stdout = old_stdout

        # Get captured output if result_text is None
        # å¦‚æœ result_text ä¸º Noneï¼Œåˆ™è·å–æ•è·çš„è¾“å‡º
        if result_text is None or not result_text.strip():
            captured_text = captured_output.getvalue()
            if captured_text.strip():
                # The model printed the result, extract it
                # æ¨¡å‹æ‰“å°äº†ç»“æœï¼Œæå–å®ƒ
                result_text = captured_text.strip()
                logger.info("Captured output from model print statements")
                logger.info("ä»æ¨¡å‹æ‰“å°è¯­å¥ä¸­æ•è·äº†è¾“å‡º")

        logger.info("âœ“ OCR completed successfully! / OCR æˆåŠŸå®Œæˆï¼")

        # Check if result_text is None or empty / æ£€æŸ¥ç»“æœæ˜¯å¦ä¸º None æˆ–ç©º
        if result_text is None or (isinstance(result_text, str) and not result_text.strip()):
            logger.warning("Model returned None or empty result")
            logger.info("Checking output directory for saved results...")
            # Try to read from the saved file / å°è¯•ä»ä¿å­˜çš„æ–‡ä»¶è¯»å–
            import glob
            latest_files = sorted(glob.glob(os.path.join(OUTPUT_DIR, "*.txt")), key=os.path.getmtime, reverse=True)
            if latest_files:
                with open(latest_files[0], 'r', encoding='utf-8') as f:
                    result_text = f.read()
                logger.info(f"Successfully read result from: {latest_files[0]}")
            else:
                result_text = "OCR completed but no text was extracted. The image may be blank or unreadable."
                logger.warning("No output files found, using default message")

        logger.info(f"\nExtracted text / æå–çš„æ–‡æœ¬:\n{result_text}")

        # Save to file if requested / å¦‚è¯·æ±‚ï¼Œä¿å­˜åˆ°æ–‡ä»¶
        output_filename = None
        if save_to_file:
            output_filename = save_text_result(result_text, prefix="ocr_result")

        # Cleanup after processing / å¤„ç†åæ¸…ç†
        cleanup_memory()

        if torch.cuda.is_available():
            mem_after = torch.cuda.memory_allocated(0) / 1024**3
            logger.info(f"GPU memory after / GPU å†…å­˜ä½¿ç”¨ï¼ˆåï¼‰: {mem_after:.2f} GB")

        logger.info("=" * 70 + "\n")

        return OCRResponse(
            success=True,
            message="OCR completed successfully / OCR æˆåŠŸå®Œæˆ",
            text=result_text,
            output_file=output_filename
        )

    except torch.cuda.OutOfMemoryError as e:
        logger.error("\n" + "=" * 70)
        logger.error("âŒ CUDA OUT OF MEMORY ERROR / CUDA å†…å­˜ä¸è¶³é”™è¯¯")
        logger.error("=" * 70)
        cleanup_memory()

        suggestions = [
            "Reduce image size before uploading / ä¸Šä¼ å‰å‡å°å›¾ç‰‡å°ºå¯¸",
            "Close all other GPU applications / å…³é—­æ‰€æœ‰å…¶ä»– GPU åº”ç”¨ç¨‹åº",
            "Restart the API server to clear memory / é‡å¯ API æœåŠ¡å™¨ä»¥æ¸…ç†å†…å­˜",
            "Try processing a smaller image first / å…ˆå°è¯•å¤„ç†è¾ƒå°çš„å›¾ç‰‡"
        ]

        logger.error("\nğŸ’¡ Suggestions / å»ºè®®:")
        for i, suggestion in enumerate(suggestions, 1):
            logger.error(f"   {i}. {suggestion}")

        raise HTTPException(
            status_code=507,
            detail={
                "error": "GPU out of memory / GPU å†…å­˜ä¸è¶³",
                "message": "The OCR process exceeded available GPU memory / OCR å¤„ç†è¶…å‡ºäº†å¯ç”¨ GPU å†…å­˜",
                "suggestions": suggestions
            }
        )
    except Exception as e:
        logger.error(f"\nâŒ Error during OCR / OCR è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        cleanup_memory()
        raise HTTPException(status_code=500, detail=f"OCR failed / OCR å¤±è´¥: {str(e)}")

@app.post("/ocr-base64", response_model=OCRResponse)
async def ocr_from_base64(request: OCRRequest):
    """
    Extract text from base64 encoded image (LOW VRAM version)
    ä» base64 ç¼–ç çš„å›¾ç‰‡ä¸­æå–æ–‡æœ¬ï¼ˆä½æ˜¾å­˜ç‰ˆæœ¬ï¼‰

    Optimized for GPUs with 6GB+ VRAM
    é’ˆå¯¹ 6GB+ æ˜¾å­˜çš„ GPU ä¼˜åŒ–
    """
    if model is None or tokenizer is None:
        raise HTTPException(
            status_code=503,
            detail="Model not loaded / æ¨¡å‹æœªåŠ è½½"
        )

    try:
        # Decode base64 image / è§£ç  base64 å›¾ç‰‡
        logger.info("\n" + "=" * 70)
        logger.info("Starting OCR (LOW VRAM MODE) / å¼€å§‹ OCRï¼ˆä½æ˜¾å­˜æ¨¡å¼ï¼‰")
        logger.info("=" * 70)

        # Cleanup before processing / å¤„ç†å‰æ¸…ç†
        cleanup_memory()

        pil_image = decode_base64_image(request.image_base64)
        logger.info(f"Image size / å›¾ç‰‡å°ºå¯¸: {pil_image.size}")
        logger.info(f"Prompt / æç¤ºè¯: {request.prompt}")

        # Save input image and get filepath / ä¿å­˜è¾“å…¥å›¾ç‰‡å¹¶è·å–æ–‡ä»¶è·¯å¾„
        image_filepath = save_image(pil_image, prefix="input")

        if torch.cuda.is_available():
            mem_before = torch.cuda.memory_allocated(0) / 1024**3
            logger.info(f"GPU memory before / GPU å†…å­˜ä½¿ç”¨ï¼ˆå‰ï¼‰: {mem_before:.2f} GB")

        # Perform OCR / æ‰§è¡Œ OCR
        logger.info("\nPerforming OCR... / æ‰§è¡Œ OCR...")
        logger.info(f"Settings / è®¾ç½®: base_size={request.base_size}, image_size={request.image_size}, crop_mode={request.crop_mode}")

        # Capture stdout to get the model's printed output
        # æ•è·æ ‡å‡†è¾“å‡ºä»¥è·å–æ¨¡å‹æ‰“å°çš„è¾“å‡º
        import io as io_module
        import sys

        old_stdout = sys.stdout
        sys.stdout = captured_output = io_module.StringIO()

        try:
            with torch.inference_mode():
                # Run inference using model.infer() / ä½¿ç”¨ model.infer() è¿è¡Œæ¨ç†
                # Use save_results=False to get return value / ä½¿ç”¨ save_results=False ä»¥è·å–è¿”å›å€¼
                logger.info("Starting model inference... / å¼€å§‹æ¨¡å‹æ¨ç†...")
                logger.info("This may take 1-3 minutes for large images / å¤§å›¾ç‰‡å¯èƒ½éœ€è¦ 1-3 åˆ†é’Ÿ")

                result_text = model.infer(
                    tokenizer,
                    prompt=request.prompt,
                    image_file=image_filepath,
                    output_path=OUTPUT_DIR,  # Use output directory / ä½¿ç”¨è¾“å‡ºç›®å½•
                    base_size=request.base_size,     # Higher values = better quality / æ›´é«˜çš„å€¼ = æ›´å¥½çš„è´¨é‡
                    image_size=request.image_size,   # Higher values = better quality / æ›´é«˜çš„å€¼ = æ›´å¥½çš„è´¨é‡
                    crop_mode=request.crop_mode,     # Enable boundary detection / å¯ç”¨è¾¹ç•Œæ£€æµ‹
                    save_results=False               # Return result instead of just saving / è¿”å›ç»“æœè€Œä¸ä»…ä»…ä¿å­˜
                )

                logger.info("Model inference completed / æ¨¡å‹æ¨ç†å®Œæˆ")
        finally:
            # Restore stdout
            sys.stdout = old_stdout

        # Get captured output if result_text is None
        # å¦‚æœ result_text ä¸º Noneï¼Œåˆ™è·å–æ•è·çš„è¾“å‡º
        if result_text is None or not result_text.strip():
            captured_text = captured_output.getvalue()
            if captured_text.strip():
                # The model printed the result, extract it
                # æ¨¡å‹æ‰“å°äº†ç»“æœï¼Œæå–å®ƒ
                result_text = captured_text.strip()
                logger.info("Captured output from model print statements")
                logger.info("ä»æ¨¡å‹æ‰“å°è¯­å¥ä¸­æ•è·äº†è¾“å‡º")

        logger.info("âœ“ OCR completed successfully! / OCR æˆåŠŸå®Œæˆï¼")

        # Check if result_text is None or empty / æ£€æŸ¥ç»“æœæ˜¯å¦ä¸º None æˆ–ç©º
        if result_text is None or (isinstance(result_text, str) and not result_text.strip()):
            logger.warning("Model returned None or empty result")
            logger.info("Checking output directory for saved results...")
            # Try to read from the saved file / å°è¯•ä»ä¿å­˜çš„æ–‡ä»¶è¯»å–
            import glob
            latest_files = sorted(glob.glob(os.path.join(OUTPUT_DIR, "*.txt")), key=os.path.getmtime, reverse=True)
            if latest_files:
                with open(latest_files[0], 'r', encoding='utf-8') as f:
                    result_text = f.read()
                logger.info(f"Successfully read result from: {latest_files[0]}")
            else:
                result_text = "OCR completed but no text was extracted. The image may be blank or unreadable."
                logger.warning("No output files found, using default message")

        logger.info(f"\nExtracted text / æå–çš„æ–‡æœ¬:\n{result_text}")

        # Save to file if requested / å¦‚è¯·æ±‚ï¼Œä¿å­˜åˆ°æ–‡ä»¶
        output_filename = None
        if request.save_to_file:
            output_filename = save_text_result(result_text, prefix="ocr_result")

        # Cleanup after processing / å¤„ç†åæ¸…ç†
        cleanup_memory()

        if torch.cuda.is_available():
            mem_after = torch.cuda.memory_allocated(0) / 1024**3
            logger.info(f"GPU memory after / GPU å†…å­˜ä½¿ç”¨ï¼ˆåï¼‰: {mem_after:.2f} GB")

        logger.info("=" * 70 + "\n")

        return OCRResponse(
            success=True,
            message="OCR completed successfully / OCR æˆåŠŸå®Œæˆ",
            text=result_text,
            output_file=output_filename
        )

    except torch.cuda.OutOfMemoryError as e:
        logger.error("\n" + "=" * 70)
        logger.error("âŒ CUDA OUT OF MEMORY ERROR / CUDA å†…å­˜ä¸è¶³é”™è¯¯")
        logger.error("=" * 70)
        cleanup_memory()

        suggestions = [
            "Reduce image size / å‡å°å›¾ç‰‡å°ºå¯¸",
            "Close all other GPU applications / å…³é—­æ‰€æœ‰å…¶ä»– GPU åº”ç”¨ç¨‹åº",
            "Restart the API server to clear memory / é‡å¯ API æœåŠ¡å™¨ä»¥æ¸…ç†å†…å­˜",
            "Try processing a smaller image first / å…ˆå°è¯•å¤„ç†è¾ƒå°çš„å›¾ç‰‡"
        ]

        logger.error("\nğŸ’¡ Suggestions / å»ºè®®:")
        for i, suggestion in enumerate(suggestions, 1):
            logger.error(f"   {i}. {suggestion}")

        raise HTTPException(
            status_code=507,
            detail={
                "error": "GPU out of memory / GPU å†…å­˜ä¸è¶³",
                "message": "The OCR process exceeded available GPU memory / OCR å¤„ç†è¶…å‡ºäº†å¯ç”¨ GPU å†…å­˜",
                "suggestions": suggestions
            }
        )
    except ValueError as e:
        logger.error(f"\nâŒ Invalid image data / æ— æ•ˆçš„å›¾ç‰‡æ•°æ®: {e}")
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"\nâŒ Error during OCR / OCR è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        cleanup_memory()
        raise HTTPException(status_code=500, detail=f"OCR failed / OCR å¤±è´¥: {str(e)}")

@app.get("/results/{filename}")
async def get_result_file(filename: str):
    """
    Download a saved OCR result file
    ä¸‹è½½ä¿å­˜çš„ OCR ç»“æœæ–‡ä»¶
    """
    filepath = os.path.join(OUTPUT_DIR, filename)

    if not os.path.exists(filepath):
        raise HTTPException(
            status_code=404,
            detail=f"File not found / æ–‡ä»¶æœªæ‰¾åˆ°: {filename}"
        )

    return FileResponse(
        filepath,
        media_type="text/plain",
        filename=filename
    )

if __name__ == "__main__":
    logger.info("\n" + "=" * 70)
    logger.info("Starting DeepSeek-OCR API Server (LOW VRAM MODE)")
    logger.info("å¯åŠ¨ DeepSeek-OCR API æœåŠ¡å™¨ï¼ˆä½æ˜¾å­˜æ¨¡å¼ï¼‰")
    logger.info("=" * 70 + "\n")

    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8031,
        log_level="info"
    )
