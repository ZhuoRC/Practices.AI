# ============================================================================
# AI Summarizer Configuration
# ============================================================================

# ----------------------------------------------------------------------------
# Server Configuration
# ----------------------------------------------------------------------------
HOST=0.0.0.0
PORT=8002

# ----------------------------------------------------------------------------
# LLM Provider Selection
# ----------------------------------------------------------------------------
# Options: "cloud" or "local"
# - cloud: Use Qwen API via Alibaba Cloud DashScope (requires API key)
# - local: Use Ollama running locally (free, requires Ollama installation)
LLM_PROVIDER=cloud

# ----------------------------------------------------------------------------
# Cloud LLM Configuration (Qwen via Alibaba Cloud DashScope)
# ----------------------------------------------------------------------------
# Required when LLM_PROVIDER=cloud
# Get your API key from: https://dashscope.console.aliyun.com/

# API Configuration
QWEN_API_KEY=your_api_key_here
QWEN_API_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1
QWEN_MODEL=qwen-turbo

# Available models: qwen-turbo, qwen-plus, qwen-max, qwen3-30b-a3b, etc.
# See: https://help.aliyun.com/zh/model-studio/getting-started/models

# ----------------------------------------------------------------------------
# Local LLM Configuration (Ollama)
# ----------------------------------------------------------------------------
# Required when LLM_PROVIDER=local
# Install Ollama from: https://ollama.com/

OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen2.5:7b-instruct

# Popular Ollama models: qwen2.5:7b-instruct, llama3:8b, mistral, etc.
# Download models: ollama pull <model_name>

# ----------------------------------------------------------------------------
# Transcription Configuration
# ----------------------------------------------------------------------------

# Transcription Provider Selection
# Options: "local" or "cloud"
# - local: OpenAI Whisper (free, runs locally, slower, requires disk space)
# - cloud: Qwen3-ASR-Flash via API (fast, requires API key, costs money)
TRANSCRIPTION_PROVIDER=local

# === Local Transcription (Whisper) Settings ===
# Model size: tiny, base, small, medium, large
# - tiny: Fastest, lowest accuracy (~75 MB)
# - base: Fast, good for most cases (~145 MB) [RECOMMENDED]
# - small: Balanced accuracy and speed (~490 MB)
# - medium: High accuracy, slower (~1.5 GB)
# - large: Best accuracy, very slow (~3 GB)
WHISPER_MODEL=small

# Device selection: auto, cpu, cuda
# - auto: Automatically detect and use GPU if available [RECOMMENDED]
# - cpu: Force CPU usage (slower but always compatible)
# - cuda: Force GPU usage (faster, requires NVIDIA GPU with CUDA)
WHISPER_DEVICE=auto

# Precision: auto, fp16, fp32
# - auto: Use FP16 on GPU, FP32 on CPU [RECOMMENDED]
# - fp16: Half precision (faster, less memory, GPU only)
# - fp32: Full precision (slower, more memory, always compatible)
WHISPER_PRECISION=auto

# === Cloud Transcription (Qwen ASR) Settings ===
# Uses same QWEN_API_KEY as above
QWEN_ASR_API_BASE_URL=https://dashscope.aliyuncs.com
QWEN_ASR_MODEL=qwen3-asr-flash

# Supported languages: English, Chinese, Arabic, French, German, Spanish,
# Italian, Portuguese, Russian, Japanese, Korean

# === File Upload Limits ===
# Maximum file size for audio/video uploads (bytes)
# 500MB = 100000000 bytes
MAX_AUDIO_FILE_SIZE=500000000

# Maximum audio duration (seconds)
# 3600 seconds = 1 hour
MAX_AUDIO_DURATION=3600

# ----------------------------------------------------------------------------
# Document Processing Configuration
# ----------------------------------------------------------------------------

# Document Storage
# All data is stored in the root data/ directory (relative to backend/)
DOCUMENT_STORAGE_PATH=../data/documents

# Text Chunking Configuration
# Larger chunks = fewer API calls = faster processing (less detailed)
# Smaller chunks = more API calls = slower processing (more detailed)

# Recommended for speed (fewer chunks, faster): 2000-3000
MIN_CHUNK_SIZE=2000
MAX_CHUNK_SIZE=3000

# Recommended for detail (more chunks, slower): 800-1200
# MIN_CHUNK_SIZE=800
# MAX_CHUNK_SIZE=1200

# Overlap between chunks (improves context continuity)
CHUNK_OVERLAP=200
