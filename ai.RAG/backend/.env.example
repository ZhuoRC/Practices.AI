# LLM Provider Configuration
# Options: "cloud" or "local"
LLM_PROVIDER=cloud

# DashScope Configuration (Aliyun Bailian - Cloud)
# Get your API key from: https://dashscope.console.aliyun.com/
QWEN_API_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1
QWEN_API_KEY=sk-your-dashscope-api-key-here
# Available models: qwen3-32b, qwen-plus, qwen-turbo, qwen-max
QWEN_MODEL=qwen3-32b

# Ollama Configuration (Local)
# Install Ollama: https://ollama.ai
# Pull model: ollama pull qwen2.5:7b-instruct
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen2.5:7b-instruct

# ChromaDB Configuration
CHROMA_PERSIST_DIRECTORY=../data/chroma_db

# PDF Storage
PDF_STORAGE_PATH=../data/pdfs

# Server Configuration
HOST=0.0.0.0
PORT=8001

# Embedding Model
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
# Batch size for embedding generation (affects memory usage and speed)
# CPU batch size: smaller value for systems with limited memory
EMBEDDING_BATCH_SIZE_CPU=16
# GPU batch size: larger value for faster processing with GPU
EMBEDDING_BATCH_SIZE_GPU=32

# Text Chunking
CHUNK_SIZE=500
CHUNK_OVERLAP=50

# RAG Configuration
TOP_K=8  # Number of chunks to retrieve for each query
# Prompt template file (in app/prompts/)
# Options: rag_system.txt (default), rag_detailed.txt, rag_chinese.txt
PROMPT_TEMPLATE_FILE=rag_system.txt
