from fastapi import FastAPI
from pydantic import BaseModel
from diffusers import DiffusionPipeline
import torch
import warnings
import uvicorn
import os

# Suppress all warnings from diffusers and transformers
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

app = FastAPI(title="Qwen Image Generation API", description="Generate images using Qwen model")

@app.get("/")
async def root():
    return {
        "message": "Qwen Image Generation API is running!",
        "docs": "/docs",
        "generate_endpoint": "/generate"
    }

print("â³ åŠ è½½è½»é‡æ¨¡å‹ Qwen/Qwen-Image ä¸­...")

# âœ… æ˜¾å­˜å‹å¥½è®¾ç½®
# æ£€æµ‹æ˜¯å¦æœ‰è¶³å¤Ÿçš„ GPU æ˜¾å­˜ï¼Œå¦‚æœä¸è¶³åˆ™ä½¿ç”¨ CPU
use_gpu = torch.cuda.is_available()

if use_gpu:
    try:
        print("ğŸ” å°è¯•ä½¿ç”¨ GPU åŠ è½½æ¨¡å‹...")
        pipe = DiffusionPipeline.from_pretrained(
            "Qwen/Qwen-Image",
            low_cpu_mem_usage=True,
            use_safetensors=True
        )
        pipe = pipe.to("cuda")

        # å¯ç”¨å†…å­˜ä¼˜åŒ–
        if hasattr(pipe, "enable_model_cpu_offload"):
            pipe.enable_model_cpu_offload()
        if hasattr(pipe, "enable_attention_slicing"):
            pipe.enable_attention_slicing(1)
        if hasattr(pipe, "enable_vae_slicing"):
            pipe.enable_vae_slicing()

        print("âœ… æ¨¡å‹å·²åŠ è½½ï¼ˆGPU æ¨¡å¼ï¼‰ï¼")
    except (torch.cuda.OutOfMemoryError, RuntimeError) as e:
        print(f"âš ï¸ GPU åŠ è½½å¤±è´¥: {str(e)}")
        print("âš ï¸ åˆ‡æ¢åˆ° CPU æ¨¡å¼...")
        use_gpu = False

if not use_gpu:
    print("ğŸ” ä½¿ç”¨ CPU åŠ è½½æ¨¡å‹...")
    pipe = DiffusionPipeline.from_pretrained(
        "Qwen/Qwen-Image",
        low_cpu_mem_usage=True,
        use_safetensors=True
    )
    pipe = pipe.to("cpu")
    print("âœ… æ¨¡å‹å·²åŠ è½½ï¼ˆCPU æ¨¡å¼ï¼‰ï¼")

class Prompt(BaseModel):
    prompt: str

@app.post("/generate")
async def generate(req: Prompt):
    print(f"ğŸ¨ ç”Ÿæˆä¸­: {req.prompt}")
    try:
        with torch.inference_mode():
            result = pipe(
                prompt=req.prompt,
                num_inference_steps=20,  # å‡å°‘æ­¥éª¤çœæ˜¾å­˜
                guidance_scale=1.0,
                height=512, width=512     # é™ä½åˆ†è¾¨ç‡çœæ˜¾å­˜
            )
        image = result.images[0]
        image.save("output.png")
        print("âœ… å›¾ç‰‡å·²ä¿å­˜ï¼šoutput.png")
        return {"message": "ok", "file": "output.png", "prompt": req.prompt}
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}")
        return {"message": "error", "error": str(e)}

if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨ Qwen Image API æœåŠ¡...")
    print("ğŸ“ API åœ°å€: http://localhost:8000")
    print("ğŸ“– æ–‡æ¡£åœ°å€: http://localhost:8000/docs")
    uvicorn.run(app, host="0.0.0.0", port=8000)
